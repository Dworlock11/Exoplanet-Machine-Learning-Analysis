{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dworlock11/Exoplanet-Machine-Learning-Analysis/blob/main/Exoplanet_Habitability_Analysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Import Statements"
      ],
      "metadata": {
        "id": "R2-EsQihQ4IL"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CgOnxV-2LYPr"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split, StratifiedKFold, RandomizedSearchCV, KFold\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.linear_model import LogisticRegression, Ridge, Lasso\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor, plot_tree\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.metrics import recall_score, make_scorer, classification_report, f1_score, mean_absolute_error, r2_score, root_mean_squared_error, mean_squared_error\n",
        "from sklearn.exceptions import ConvergenceWarning\n",
        "from sklearn.inspection import permutation_importance\n",
        "from warnings import simplefilter\n",
        "from scipy.stats import randint, uniform\n",
        "\n",
        "df = pd.read_excel(\"Exoplanet Catalog.xlsx\")\n",
        "pd.set_option('display.max_columns', None)\n",
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Preprocessing"
      ],
      "metadata": {
        "id": "kZGvaz5xAdsa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As many of the columns from the dataset contain a lot of null entries, it is best to simply remove them. All columns with the number of null values greater than a quarter the length of the dataset are removed."
      ],
      "metadata": {
        "id": "efFiyh0oAjWe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "col_non_null_count = df.isna().sum()\n",
        "cols_non_majority_null = col_non_null_count[col_non_null_count < len(df)/4].index.to_list()\n",
        "df = df[cols_non_majority_null]"
      ],
      "metadata": {
        "id": "nN9F605vRCHb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additional feature selection is conducted, as many of the features are unhelpful for model training, are copies of one another, or are close in value."
      ],
      "metadata": {
        "id": "JLPC33tqhIQh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop([\"P_NAME\", \"P_STATUS\", \"P_RADIUS\", \"P_YEAR\", \"P_UPDATED\", \"S_NAME\", \"S_RADIUS\", \"S_ALT_NAMES\", \"P_HABZONE_OPT\", \"P_HABZONE_CON\", \"S_CONSTELLATION_ABR\", \"P_PERIOD_ERROR_MIN\", \"P_PERIOD_ERROR_MAX\", \"S_DISTANCE_ERROR_MIN\", \"S_DISTANCE_ERROR_MAX\", \"P_FLUX_MIN\", \"P_FLUX_MAX\", \"P_TEMP_EQUIL_MIN\", \"P_TEMP_EQUIL_MAX\"], axis=1)\n",
        "df.shape"
      ],
      "metadata": {
        "id": "-Ll_3aw0dJyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Categorical features with far too many unique values are removed to simplify the model after encoding."
      ],
      "metadata": {
        "id": "4ba8S0Vylt40"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "num_features = df.select_dtypes(include=np.number)\n",
        "cat_features = df.select_dtypes(exclude=np.number)\n",
        "\n",
        "for col in cat_features.columns:\n",
        "  print(col, \"-\", len(cat_features[col].value_counts()))\n",
        "\n",
        "df = df.drop([\"S_RA_T\", \"S_DEC_T\", \"S_CONSTELLATION\", \"S_CONSTELLATION_ENG\"], axis=1)"
      ],
      "metadata": {
        "id": "N43DQAQ0k4UB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is checked for the skew of each feature to determine the appropriate imputing method. Since the data is heavily skewed, the median will be chosen."
      ],
      "metadata": {
        "id": "3efzb0Ke3wOa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.skew(axis=0, numeric_only=True, skipna=True).sort_values(ascending=False)"
      ],
      "metadata": {
        "id": "xrt7HJljuQA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The distribution of the classification target is observed."
      ],
      "metadata": {
        "id": "ekQnl3rbjPXr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"P_TYPE\"].value_counts()"
      ],
      "metadata": {
        "id": "Xnsyp8tdhHtw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A single Miniterran planet can't be split amongst a training and test set. According to the official classification practice of exoplanets, Miniterrans have a radius between 0.03 and 0.04 times that of Earth. Subterrans have a radius between 0.4 and 0.8 times that of Earth. If the Miniterran in the data has a radius close to that of Subterrans, it wouldn't be a problem to mask it as one."
      ],
      "metadata": {
        "id": "bXpgnEr4jUyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "miniterran = df[df[\"P_TYPE\"] == \"Miniterran\"]\n",
        "miniterran[\"P_RADIUS_EST\"]"
      ],
      "metadata": {
        "id": "hU1D_AdZiITd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Indeed, the radius is around 0.33 times that of Earth, which isn't too far from the 0.4 minimum for a Subterran. Therefore, the planet is masked as one."
      ],
      "metadata": {
        "id": "EMxLhyGIlRIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"P_TYPE\"] = df[\"P_TYPE\"].mask(df[\"P_TYPE\"] == \"Miniterran\", \"Subterran\")\n",
        "df[\"P_TYPE\"].value_counts()"
      ],
      "metadata": {
        "id": "yKjXkBoiimkT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the distribution of the target for the regression models is analyzed."
      ],
      "metadata": {
        "id": "Xy_05OuwU2qk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[\"P_MASS_EST\"].describe()"
      ],
      "metadata": {
        "id": "hC6PrHIiU2Um"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.plot(df.index, df[\"P_MASS_EST\"].sort_values(ascending=False))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "lJ7V_EnzaKzx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It's not clear what exactly it means for a planet to have a mass of 0.0. It might be a mistake. Such entries will be removed to be safe."
      ],
      "metadata": {
        "id": "KXquGmbiZKI6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = df[df[\"P_MASS_EST\"] != 0.0]"
      ],
      "metadata": {
        "id": "dC3QO5-XZZLI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Additionally, the smallest planets and the largest are orders of magnitude apart. Therefore, it would make sense to tranform the mass to be in log space.\n",
        "\n",
        "Mean Absolute Error (MAE) will be used to evaluate the model, as it is robust against outliers, of which the data has a lot."
      ],
      "metadata": {
        "id": "apu9DaaxhLMx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression"
      ],
      "metadata": {
        "id": "iiykhAo8LjrA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is separated into the features and the target."
      ],
      "metadata": {
        "id": "T_Rg5LPslckk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop([\"P_TYPE\"], axis=1)\n",
        "y = df[\"P_TYPE\"]"
      ],
      "metadata": {
        "id": "0tpMgBdWwINt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All rows where the target value is null are removed."
      ],
      "metadata": {
        "id": "eZ57GriKQlqz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_na = y[y.isna()]\n",
        "data = X.join(y)\n",
        "data = data.drop(y_na.index)\n",
        "X = data.drop(\"P_TYPE\", axis=1)\n",
        "y = data[\"P_TYPE\"]\n",
        "print(y.isna().sum())"
      ],
      "metadata": {
        "id": "eHXql9vzQeXE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into the training and testing data."
      ],
      "metadata": {
        "id": "PoOzUuQBC3Ec"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=9)"
      ],
      "metadata": {
        "id": "G3yeJMs5b71e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers for numerical and categorical data are created."
      ],
      "metadata": {
        "id": "erZ1cs0gdos5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate numerical and categorical features\n",
        "num_features = X_train.select_dtypes(include=np.number)\n",
        "cat_features = X_train.select_dtypes(exclude=np.number)\n",
        "num_col_names = num_features.columns\n",
        "cat_col_names = cat_features.columns\n",
        "\n",
        "# Build transformers\n",
        "num_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "ohe_cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OneHotEncoder(handle_unknown=\"ignore\", feature_name_combiner=\"concat\"))\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "log_preprocessor = ColumnTransformer([\n",
        "    (\"num_transformer\", num_transformer, num_col_names),\n",
        "    (\"ohe_cat_transformer\", ohe_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "i4tHd_DFuqqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "WgfPhPBJIoge"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_pipe = Pipeline([\n",
        "    (\"log_preprocessor\", log_preprocessor),\n",
        "    (\"log_reg\", LogisticRegression(\n",
        "        solver=\"lbfgs\",\n",
        "        penalty=\"l2\",\n",
        "        max_iter=300\n",
        "    ))\n",
        "])\n",
        "\n",
        "kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "param_dist = {\n",
        "    \"log_reg__C\": np.logspace(-3, 3, 15),\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(log_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "bQfNnP1WLFjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "LKNJN3aJWj7z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "AtJB1rjAWh2S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "a3Yr8IrouQT3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs very well across all metrics.\n",
        "\n",
        "Permutation is used to find the importance of the individual features. It will be used across all models for standardized results. The test set must be manually transformed with all preprocessing steps before implementing permutation to match the number of columns present in the model."
      ],
      "metadata": {
        "id": "OdQ-FyR6Lteq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"log_preprocessor\"]\n",
        "log_reg = best_model.named_steps[\"log_reg\"]\n",
        "feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "raw_feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "clean_feature_names = [\n",
        "    name.split(\"__\", 1)[1] if \"__\" in name else name\n",
        "    for name in raw_feature_names\n",
        "]\n",
        "\n",
        "# Transform X_test into expanded feature space\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(log_reg, X_test_transformed, y_test, n_repeats=10, random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Logistic Regression Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "wJRJnJQonIZ1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Apparently, the most important feature for predicting the type of the planet is P_RADIUS_EST. This make sense, as the classification of a planet is based on the planet's radius compared to Earth's."
      ],
      "metadata": {
        "id": "1kGF9684yMhA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Logistic Regression"
      ],
      "metadata": {
        "id": "olAE19ILdK2L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now polynomial features will be added to see if there will be a significant difference.\n",
        "\n",
        "Transformers are created once again."
      ],
      "metadata": {
        "id": "OEKoLGN-dTaK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Build transformers\n",
        "poly_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"poly\", PolynomialFeatures(degree=2, include_bias=False)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "poly_log_preprocessor = ColumnTransformer([\n",
        "    (\"poly_transformer\", poly_transformer, num_col_names),\n",
        "    (\"ohe_cat_transformer\", ohe_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "vYtB0dpXdK2N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "sNZjPnKhdK2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_log_pipe = Pipeline([\n",
        "    (\"poly_log_preprocessor\", poly_log_preprocessor),\n",
        "    (\"log_reg\", LogisticRegression(\n",
        "        solver=\"lbfgs\",\n",
        "        penalty=\"l2\",\n",
        "        max_iter=300\n",
        "    ))\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    # \"log_reg__C\": np.logspace(-3, 3, 15),\n",
        "    \"log_reg__C\" : [51.794746792312125]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(poly_log_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "GVce-65zdK2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "usBLrDqZdK2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# simplefilter(\"ignore\", category=ConvergenceWarning)\n",
        "\n",
        "# search.fit(X_train, y_train)\n",
        "# best_model = search.best_estimator_\n",
        "# for param, value in search.best_params_.items():\n",
        "#   print(param,\":\", value)\n",
        "\n",
        "# y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "op-TYN3zdK2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "9eqbaj-rdK2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs around the same as without polynomial features. However, the time necesary to fit is significantly longer. Therefore, there seems to be little reason to use polynomial logistic regression.\n",
        "\n",
        "Feature importance is ignored, as most of the features are simply engineered polynomial features."
      ],
      "metadata": {
        "id": "44pwfPrxdK2P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree"
      ],
      "metadata": {
        "id": "DRDkkz5wGiV1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, a decision tree model will be trained following the same process.\n",
        "\n",
        "A new categorical transformer is created using ordinal encoding, which is suitable for tree-based models."
      ],
      "metadata": {
        "id": "VFqdH5Uv-IEF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_cat_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"encoder\", OrdinalEncoder(handle_unknown=\"use_encoded_value\", unknown_value=-1))\n",
        "])\n",
        "\n",
        "# Combine transformers\n",
        "tree_preprocessor = ColumnTransformer([\n",
        "    (\"num_transformer\", num_transformer, num_col_names),\n",
        "    (\"tree_cat_transformer\", tree_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "YgB5cVXmjDwj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented, testing ranges of values for the major hyperparameters."
      ],
      "metadata": {
        "id": "bK5N2b9b-V9D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_pipe = Pipeline([\n",
        "    (\"tree_preprocessor\", tree_preprocessor),\n",
        "    (\"dec_tree\", DecisionTreeClassifier())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"dec_tree__max_depth\": [None, 2, 5, 10, 20],\n",
        "    \"dec_tree__min_samples_split\": [2, 5, 10, 20, 50],\n",
        "    \"dec_tree__min_samples_leaf\": [1, 2, 5, 10, 20],\n",
        "    \"dec_tree__max_features\": [\"sqrt\", \"log2\", None],\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "QTZIsZjD-V9D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "cJ8RShBtfPk5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "FXe7j2xefPk7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "BZd2d3rXHQ76"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics are notably better than those from the logistic regression model. Perhaps decision trees are better suited to multiclass classification.\n",
        "\n",
        "Permutation is once again used to discover feature importance."
      ],
      "metadata": {
        "id": "UGo257n_fPk9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"tree_preprocessor\"]\n",
        "dec_tree = best_model.named_steps[\"dec_tree\"]\n",
        "\n",
        "# Remove name of transformer from each feature\n",
        "raw_feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "clean_feature_names = [\n",
        "    name.split(\"__\", 1)[1] if \"__\" in name else name\n",
        "    for name in raw_feature_names\n",
        "]\n",
        "\n",
        "# Transform X_test into expanded feature space\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(dec_tree, X_test_transformed, y_test, n_repeats=10, random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Decision Tree Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ddH-8vqhfPk9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In comparison to the logistic regression model, P_MASS_EST has more significance for prediction. The planet's radius is still the most important predictor, however."
      ],
      "metadata": {
        "id": "ax3REue5H_yw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest"
      ],
      "metadata": {
        "id": "oaK_VHp0Phmn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, a random forest model will be trained. The pipeline is created and hyperparameter tuning is implemented, testing ranges of values for the major hyperparameters."
      ],
      "metadata": {
        "id": "tGu3vtUCPhmp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# forest_pipe = Pipeline([\n",
        "#     (\"tree_preprocessor\", tree_preprocessor),\n",
        "#     (\"rand_for\", RandomForestClassifier())\n",
        "# ])\n",
        "\n",
        "# param_dist = {\n",
        "#     \"rand_for__n_estimators\": [200, 400, 600, 800],\n",
        "#     \"rand_for__max_depth\": [None, 5, 10, 20, 40],\n",
        "#     \"rand_for__min_samples_split\": [2, 5, 10, 20],\n",
        "#     \"rand_for__min_samples_leaf\": [1, 2, 5, 10],\n",
        "#     \"rand_for__max_features\": [\"sqrt\", \"log2\", None],\n",
        "#     \"rand_for__bootstrap\": [True, False],\n",
        "# }\n",
        "\n",
        "# search = RandomizedSearchCV(forest_pipe, param_distributions=param_dist, n_iter=10, cv=kf, random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "I2L0QecXPhmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "oWtZktC2Phmq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# search.fit(X_train, y_train)\n",
        "# best_model = search.best_estimator_\n",
        "# for param, value in search.best_params_.items():\n",
        "#   print(param,\":\", value)\n",
        "\n",
        "# y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "q-VU6MKDPhmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "5DVZprO8Phmq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The metrics are around the same as those for the decision tree model. However, it takes much longer to fit, making random forests apparently unnecessary for planet classification.\n",
        "\n",
        "Permutation is once again used to discover feature importance."
      ],
      "metadata": {
        "id": "4UEgwA6YPhmr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # Extract components\n",
        "# preprocessor = best_model.named_steps[\"tree_preprocessor\"]\n",
        "# rand_for = best_model.named_steps[\"rand_for\"]\n",
        "\n",
        "# # Transform X_test into expanded feature space\n",
        "# X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# # Run permutation importance on the classifier only\n",
        "# importances = permutation_importance(rand_for, X_test_transformed, y_test, n_repeats=10, random_state=9, n_jobs=-1)\n",
        "\n",
        "# # Display results\n",
        "# highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "# plt.bar(highest_importances.index, highest_importances)\n",
        "# plt.xticks(rotation=90)\n",
        "# plt.title(\"Random Forest Feature Importance\")\n",
        "# plt.xlabel(\"Feature\")\n",
        "# plt.ylabel(\"Drop in Performance\")\n",
        "# plt.show()"
      ],
      "metadata": {
        "id": "cVTXxbd0Phmr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As before, P_RADIUS_EST and P_MASS_EST are the two most important features. As with the decision tree model, the other features have little importance compared to those two."
      ],
      "metadata": {
        "id": "Q6Y4-CGpPhmr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ridge Regression"
      ],
      "metadata": {
        "id": "_PrCnd1raZuC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, the mass of planets will be predicted using various models, starting with Ridge regression. Ridge is chosen over standard linear regression to enable regularization.\n",
        "\n",
        "As mentioned earlier, P_MASS_EST is transformed to be in log space."
      ],
      "metadata": {
        "id": "VGvL83QjTVsu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "log_df = df.copy()\n",
        "log_df[\"Log_Mass\"] = np.log10(log_df[\"P_MASS_EST\"])\n",
        "log_df = log_df.drop(\"P_MASS_EST\", axis=1)"
      ],
      "metadata": {
        "id": "OKY65wh6egXx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into the features and the target."
      ],
      "metadata": {
        "id": "ARfjM4CHlQ9A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = log_df.drop(\"Log_Mass\", axis=1)\n",
        "y = log_df[\"Log_Mass\"]"
      ],
      "metadata": {
        "id": "RNFJsujOTVsu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All rows where the target value is null are removed."
      ],
      "metadata": {
        "id": "ZGWnRr8RTVsv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_na = y[y.isna()]\n",
        "data = X.join(y)\n",
        "data = data.drop(y_na.index)\n",
        "X = data.drop(\"Log_Mass\", axis=1)\n",
        "y = data[\"Log_Mass\"]\n",
        "print(y.isna().sum())"
      ],
      "metadata": {
        "id": "3uHcwNfnTVsv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into the training and testing data."
      ],
      "metadata": {
        "id": "pS4FisKyTVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
      ],
      "metadata": {
        "id": "X2gemHJOTVsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transformers for numerical and categorical data are created."
      ],
      "metadata": {
        "id": "jSkXup1xTVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate numerical and categorical features\n",
        "num_features = X_train.select_dtypes(include=np.number)\n",
        "cat_features = X_train.select_dtypes(exclude=np.number)\n",
        "num_col_names = num_features.columns\n",
        "cat_col_names = cat_features.columns\n",
        "\n",
        "# Combine transformers\n",
        "ridge_preprocessor = ColumnTransformer([\n",
        "    (\"num_transformer\", num_transformer, num_col_names),\n",
        "    (\"ohe_cat_transformer\", ohe_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "jYGtSBVrTVsw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "YC3yEWP8TVsw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ridge_pipe = Pipeline([\n",
        "    (\"ridge_preprocessor\", ridge_preprocessor),\n",
        "    (\"ridge\", Ridge())\n",
        "])\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=9)\n",
        "\n",
        "param_dist = {\n",
        "    \"ridge__alpha\": np.logspace(-4, 4)\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(ridge_pipe, param_distributions=param_dist, scoring=\"neg_root_mean_squared_error\", n_iter=50, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "Kz0vzL_DTVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "o0VWc2nDTVsx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "7N4iPNCiTVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_se = pd.Series(y_pred)\n",
        "y_pred_se.describe()"
      ],
      "metadata": {
        "id": "mMuFof7grPN4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_test_se = pd.Series(y_test)\n",
        "y_test_se.describe()"
      ],
      "metadata": {
        "id": "8YgaRjGyw64K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE is used to evaluate the model."
      ],
      "metadata": {
        "id": "XO4y-TngjNyt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "VOzeSBzQTVsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs reasonably well across all metrics."
      ],
      "metadata": {
        "id": "t9pJSoRATVsy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = RandomizedSearchCV(ridge_pipe, param_distributions=param_dist, scoring=\"neg_mean_absolute_error\", n_iter=50, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "w7CZtwO_Db-d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "rKsaHHFpDb-e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "7rJgSXCTDb-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_pred_se = pd.Series(y_pred)\n",
        "y_pred_se.describe()"
      ],
      "metadata": {
        "id": "t9bBGRCBxacE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE is used to evaluate the model."
      ],
      "metadata": {
        "id": "U8EN-84JDb-f"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "oM7f3B6YDb-f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Polynomial Ridge Regression"
      ],
      "metadata": {
        "id": "CEhW39HhEmL4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output from Ridge will be compared to its output with polynomial features.\n",
        "\n",
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "zvvpWO5XEmL5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "poly_transformer = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"poly\", PolynomialFeatures(include_bias=False)),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "poly_ridge_preprocessor = ColumnTransformer([\n",
        "    (\"poly_transformer\", poly_transformer, num_col_names),\n",
        "    (\"ohe_cat_transformer\", ohe_cat_transformer, cat_col_names)\n",
        "])"
      ],
      "metadata": {
        "id": "xzklWn8PF8vF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "poly_ridge_pipe = Pipeline([\n",
        "    (\"poly_ridge_preprocessor\", poly_ridge_preprocessor),\n",
        "    (\"ridge\", Ridge())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"poly_ridge_preprocessor__poly_transformer__poly__degree\" : [2, 3],\n",
        "    \"ridge__alpha\" : np.logspace(-4, 4)\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(poly_ridge_pipe, param_distributions=param_dist, scoring=\"neg_root_mean_squared_error\", n_iter=10, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "j0lqiFlXEmL9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "OUJzOxYtEmL-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# search.fit(X_train, y_train)\n",
        "# best_model = search.best_estimator_\n",
        "# for param, value in search.best_params_.items():\n",
        "#   print(param,\":\", value)\n",
        "\n",
        "# y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "CEL7V5xjEmL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_se = pd.Series(y_pred)\n",
        "# y_pred_se.describe()"
      ],
      "metadata": {
        "id": "9vOGiC6Bxewi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_se.sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "id": "jKMNmb-Lxewj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE and R-squared scores are used to evaluate the model."
      ],
      "metadata": {
        "id": "hSRvyUuSi-T1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "Q_PT_UoXEmL-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs reasonably well across all metrics."
      ],
      "metadata": {
        "id": "8C9sePViEmL_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# search = RandomizedSearchCV(poly_ridge_pipe, param_distributions=param_dist, scoring=\"neg_mean_absolute_error\", n_iter=10, cv=kf,\n",
        "#                             random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "QAi8cfejlAgy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# search.fit(X_train, y_train)\n",
        "# best_model = search.best_estimator_\n",
        "# for param, value in search.best_params_.items():\n",
        "#   print(param,\":\", value)\n",
        "\n",
        "# y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "4QXCx5njlGNJ"
      },
      "execution_count": 822,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_se = pd.Series(y_pred)\n",
        "# y_pred_se.describe()"
      ],
      "metadata": {
        "id": "In9MXDcmxhrq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# y_pred_se.sort_values(ascending=False).head(10)"
      ],
      "metadata": {
        "id": "IK7BYUVlxhrr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "UotaO_OVlIFl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Regressor"
      ],
      "metadata": {
        "id": "aGeGi1Tr3ZgW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "_TS-PEVd3U2L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = df.drop(\"P_MASS_EST\", axis=1)\n",
        "y = df[\"P_MASS_EST\"]"
      ],
      "metadata": {
        "id": "qwLQ_0KL-Qu-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "All rows where the target value is null are removed."
      ],
      "metadata": {
        "id": "SZO-Vc8n-Qu_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_na = y[y.isna()]\n",
        "data = X.join(y)\n",
        "data = data.drop(y_na.index)\n",
        "X = data.drop(\"P_MASS_EST\", axis=1)\n",
        "y = data[\"P_MASS_EST\"]\n",
        "print(y.isna().sum())"
      ],
      "metadata": {
        "id": "-w3PqPnV-QvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The data is split into the training and testing data."
      ],
      "metadata": {
        "id": "ZoG-KTMb-QvB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=9)"
      ],
      "metadata": {
        "id": "sUtXBl6D-QvB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tree_pipe = Pipeline([\n",
        "    (\"tree_preprocessor\", tree_preprocessor),\n",
        "    (\"dec_tree\", DecisionTreeRegressor())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"dec_tree__max_depth\": [None, 3, 5, 10, 20],\n",
        "    \"dec_tree__min_samples_split\": [2, 5, 10, 20, 50],\n",
        "    \"dec_tree__min_samples_leaf\": [1, 2, 5, 10, 20, 50],\n",
        "    \"dec_tree__max_features\": [None, \"sqrt\", \"log2\"]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, scoring=\"neg_root_mean_squared_error\", n_iter=50, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "B75KmyC-3U2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "ygGHkX-E3U2M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "2_FZXof23U2M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE is used to evaluate the model."
      ],
      "metadata": {
        "id": "qNnkpNsE3U2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "7xapZkxh3U2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"tree_preprocessor\"]\n",
        "dec_tree = best_model.named_steps[\"dec_tree\"]\n",
        "\n",
        "# Remove name of transformer from each feature\n",
        "raw_feature_names = preprocessor.get_feature_names_out()\n",
        "\n",
        "clean_feature_names = [\n",
        "    name.split(\"__\", 1)[1] if \"__\" in name else name\n",
        "    for name in raw_feature_names\n",
        "]\n",
        "\n",
        "# Transform X_test into expanded feature space\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(dec_tree, X_test_transformed, y_test, scoring=\"neg_root_mean_square_error\" n_repeats=10,\n",
        "                                     random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Decision Tree Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UHd1LwJ049wW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs reasonably well across all metrics."
      ],
      "metadata": {
        "id": "t9xVdLiA3U2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, scoring=\"neg_mean_absolute_error\", n_iter=50, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "reV8-b4v3U2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "-6P9TpAb3U2O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "CqvTAyfj3U2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE is used to evaluate the model."
      ],
      "metadata": {
        "id": "bg_Ophd73U2P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "KGBQG_FO3U2P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "dec_tree = best_model.named_steps[\"dec_tree\"]\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(dec_tree, X_test_transformed, y_test, scoring=\"neg_mean_absolute_error\" n_repeats=10,\n",
        "                                     random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Decision Tree Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "OuvB8ruv6Ck3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Regressor"
      ],
      "metadata": {
        "id": "Mkl2r0Ly7BPU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The pipeline is created and hyperparameter tuning is implemented."
      ],
      "metadata": {
        "id": "M_C_H1Iq7BPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tree_pipe = Pipeline([\n",
        "    (\"tree_preprocessor\", tree_preprocessor),\n",
        "    (\"rand_for\", RandomForestRegressor())\n",
        "])\n",
        "\n",
        "param_dist = {\n",
        "    \"rand_for__n_estimators\": [200, 300, 500, 800],\n",
        "    \"rand_for__max_depth\": [None, 5, 10, 20, 40],\n",
        "    \"rand_for__min_samples_split\": [2, 5, 10, 20],\n",
        "    \"rand_for__min_samples_leaf\": [1, 2, 5, 10, 20],\n",
        "    \"rand_for__max_features\": [\"sqrt\", \"log2\", None],\n",
        "    \"rand_for__bootstrap\": [True, False]\n",
        "}\n",
        "\n",
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, scoring=\"neg_root_mean_squared_error\", n_iter=50, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "TqSvea-_7BPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "ID5szs6N7BPV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "ENsqdSjW7BPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE is used to evaluate the model."
      ],
      "metadata": {
        "id": "EHWHswYE7BPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"RMSE:\", root_mean_squared_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "14K-LxUu7BPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "preprocessor = best_model.named_steps[\"tree_preprocessor\"]\n",
        "rand_for = best_model.named_steps[\"rand_for\"]\n",
        "\n",
        "# Transform X_test into expanded feature space\n",
        "X_test_transformed = preprocessor.transform(X_test)\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(rand_for, X_test_transformed, y_test, scoring=\"neg_root_mean_square_error\" n_repeats=10,\n",
        "                                     random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a3CrmqXc7BPW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model performs reasonably well across all metrics."
      ],
      "metadata": {
        "id": "6zj238fF7BPW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search = RandomizedSearchCV(tree_pipe, param_distributions=param_dist, scoring=\"neg_mean_absolute_error\", n_iter=50, cv=kf,\n",
        "                            random_state=9, n_jobs=-1)"
      ],
      "metadata": {
        "id": "j97eqVwX7BPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is trained, tested, and scored with a classification report."
      ],
      "metadata": {
        "id": "toG3UStP7BPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "search.fit(X_train, y_train)\n",
        "best_model = search.best_estimator_\n",
        "for param, value in search.best_params_.items():\n",
        "  print(param,\":\", value)\n",
        "\n",
        "y_pred = best_model.predict(X_test)"
      ],
      "metadata": {
        "id": "Am1muHDp7BPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The MAE is used to evaluate the model."
      ],
      "metadata": {
        "id": "IhJIFK_L7BPX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAE:\", mean_absolute_error(y_test, y_pred))"
      ],
      "metadata": {
        "id": "PvxjoIVW7BPX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract components\n",
        "rand_for = best_model.named_steps[\"rand_for\"]\n",
        "\n",
        "# Run permutation importance on the classifier only\n",
        "importances = permutation_importance(rand_for, X_test_transformed, y_test, scoring=\"neg_mean_absolute_error\" n_repeats=10,\n",
        "                                     random_state=9, n_jobs=-1)\n",
        "\n",
        "# Display results\n",
        "highest_importances = pd.Series(importances.importances_mean, index=clean_feature_names).sort_values(ascending=False).head(10)\n",
        "plt.bar(highest_importances.index, highest_importances)\n",
        "plt.xticks(rotation=90)\n",
        "plt.title(\"Random Forest Feature Importance\")\n",
        "plt.xlabel(\"Feature\")\n",
        "plt.ylabel(\"Drop in Performance\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "NXy9Tltt7BPX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}